@article{mahdavinejad_machine_2018,
	title = {Machine learning for internet of things data analysis: a survey},
	volume = {4},
	issn = {23528648},
	shorttitle = {Machine learning for internet of things data analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S235286481730247X},
	doi = {10.1016/j.dcan.2017.10.002},
	language = {en},
	number = {3},
	urldate = {2021-06-07},
	journal = {Digital Communications and Networks},
	author = {Mahdavinejad, Mohammad Saeid and Rezvan, Mohammadreza and Barekatain, Mohammadamin and Adibi, Peyman and Barnaghi, Payam and Sheth, Amit P.},
	month = aug,
	year = {2018},
	pages = {161--175},
}

@article{chen_editorial_2013,
	title = {Editorial {Low}-{Power}, {Intelligent}, and {Secure} {Solutions} for {Realization} of {Internet} of {Things}},
	volume = {3},
	issn = {2156-3357, 2156-3365},
	url = {http://ieeexplore.ieee.org/document/6472113/},
	doi = {10.1109/JETCAS.2013.2244771},
	number = {1},
	urldate = {2021-06-07},
	journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
	author = {Chen, Yen-Kuang and Wu, An-Yeu and Bayoumi, Magdy A. and Koushanfar, Farinaz},
	month = mar,
	year = {2013},
	pages = {1},
}

@inproceedings{wang_when_2018,
	address = {Honolulu, HI},
	title = {When {Edge} {Meets} {Learning}: {Adaptive} {Control} for {Resource}-{Constrained} {Distributed} {Machine} {Learning}},
	isbn = {9781538641286},
	shorttitle = {When {Edge} {Meets} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8486403/},
	doi = {10.1109/INFOCOM.2018.8486403},
	urldate = {2021-06-07},
	booktitle = {{IEEE} {INFOCOM} 2018 - {IEEE} {Conference} on {Computer} {Communications}},
	publisher = {IEEE},
	author = {Wang, Shiqiang and Tuor, Tiffany and Salonidis, Theodoros and Leung, Kin K. and Makaya, Christian and He, Ting and Chan, Kevin},
	month = apr,
	year = {2018},
	pages = {7,8},
}

@article{McMahan_Moore_Ramage_Hampson_Arcas_2017, title={Communication-Efficient Learning of Deep Networks from Decentralized Data}, abstractNote={Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.}, note={arXiv: 1602.05629}, journal={arXiv:1602.05629 [cs]}, author={McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Agüera y}, year={2017}, month={Feb}, pages={9} }
 
@article{Li_Sahu_Talwalkar_Smith_2020, title={Federated Learning: Challenges, Methods, and Future Directions}, volume={37}, ISSN={1053-5888, 1558-0792}, DOI={10.1109/MSP.2020.2975749}, abstractNote={Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.}, note={arXiv: 1908.07873}, number={3}, journal={IEEE Signal Processing Magazine}, author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia}, year={2020}, month={May}, pages={9} }

@article{Lim_Luong_Hoang_Jiao_Liang_Yang_Niyato_Miao_2020, title={Federated Learning in Mobile Edge Networks: A Comprehensive Survey}, abstractNote={In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications. Traditional cloudbased Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL}, note={arXiv: 1909.11875}, journal={arXiv:1909.11875 [cs, eess]}, author={Lim, Wei Yang Bryan and Luong, Nguyen Cong and Hoang, Dinh Thai and Jiao, Yutao and Liang, Ying-Chang and Yang, Qiang and Niyato, Dusit and Miao, Chunyan}, year={2020}, month={Feb}, pages={4,9}, }

@article{Nishio_Yonetani_2018, title={Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge}, DOI={10.1109/ICC.2019.8761315}, abstractNote={We envision a mobile edge computing (MEC) framework for machine learning (ML) technologies, which leverages distributed client data and computation resources for training high-performance ML models while preserving client privacy. Toward this future goal, this work aims to extend Federated Learning (FL), a decentralized learning framework that enables privacy-preserving training of models, to work with heterogeneous clients in a practical cellular network. The FL protocol iteratively asks random clients to download a trainable model from a server, update it with own data, and upload the updated model to the server, while asking the server to aggregate multiple client updates to further improve the model. While clients in this protocol are free from disclosing own private data, the overall training process can become inefficient when some clients are with limited computational resources (i.e. requiring longer update time) or under poor wireless channel conditions (longer upload time). Our new FL protocol, which we refer to as FedCS, mitigates this problem and performs FL efficiently while actively managing clients based on their resource conditions. Specifically, FedCS solves a client selection problem with resource constraints, which allows the server to aggregate as many client updates as possible and to accelerate performance improvement in ML models. We conducted an experimental evaluation using publicly-available large-scale image datasets to train deep neural networks on MEC environment simulations. The experimental results show that FedCS is able to complete its training process in a significantly shorter time compared to the original FL protocol.}, note={arXiv: 1804.08333}, journal={arXiv:1804.08333 [cs]}, author={Nishio, Takayuki and Yonetani, Ryo}, year={2018}, month={Oct}, pages={18,20} }

@article{Liu_Zhang_Song_Letaief_2019, title={Client-Edge-Cloud Hierarchical Federated Learning}, abstractNote={Federated Learning is a collaborative machine learning framework to train a deep learning model without accessing clients’ private data. Previous works assume one central parameter server either at the cloud or at the edge. The cloud server can access more data but with excessive communication overhead and long latency, while the edge server enjoys more efficient communications with the clients. To combine their advantages, we propose a client-edge-cloud hierarchical Federated Learning system, supported with a HierFAVG algorithm that allows multiple edge servers to perform partial model aggregation. In this way, the model can be trained faster and better communication-computation trade-offs can be achieved. Convergence analysis is provided for HierFAVG and the effects of key parameters are also investigated, which lead to qualitative design guidelines. Empirical experiments verify the analysis and demonstrate the benefits of this hierarchical architecture in different data distribution scenarios. Particularly, it is shown that by introducing the intermediate edge servers, the model training time and the energy consumption of the end devices can be simultaneously reduced compared to cloud-based Federated Learning.}, author={Liu, Lumin and Zhang, Jun and Song, S. H. and Letaief, Khaled B.}, year={2019}, month={May}, pages={7,10} }

@inproceedings{WANG_WANG_LI_2019, title={CMFL: Mitigating Communication Overhead for Federated Learning}, ISSN={2575-8411}, DOI={10.1109/ICDCS.2019.00099}, abstractNote={Federated Learning enables mobile users to collaboratively learn a global prediction model by aggregating their individual updates without sharing the privacy-sensitive data. As mobile devices usually have limited data plan and slow network connections to the central server where the global model is maintained, mitigating the communication overhead is of paramount importance. While existing works mainly focus on reducing the total bits transferred in each update via data compression, we study an orthogonal approach that identifies irrelevant updates made by clients and precludes them from being uploaded for reduced network footprint. Following this idea, we propose Communication-Mitigated Federated Learning (CMFL) in this paper. CMFL provides clients with feedback information regarding the global tendency of model updating. Each client checks if its update aligns with this global tendency and is relevant enough to model improvement. By avoiding uploading those irrelevant updates to the server, CMFL can substantially reduce the communication overhead while still guaranteeing the learning convergence. CMFL is shown to achieve general improvement in communication efficiency for almost all of the existing federated learning schemes. We evaluate CMFL through extensive simulations and EC2 emulations. Compared with vanilla Federated Learning, CMFL yields 13.97x communication efficiency in terms of the reduction of network footprint. When applied to Federated Multi-Task Learning, CMFL improves the communication efficiency by 5.7x with 4% higher prediction accuracy.}, booktitle={2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)}, author={WANG, Luping and WANG, Wei and LI, Bo}, year={2019}, month={Jul}, pages={7,10} }

@inproceedings{Tao_Li_2018, title={eSGD: Communication Efficient Distributed Deep Learning on the Edge}, url={https://www.usenix.org/conference/hotedge18/presentation/tao}, author={Tao, Zeyi and Li, Qun}, year={2018}, pages={7,10} }

@article{Lin_Han_Mao_Wang_Dally_2020, title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training}, abstractNote={Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile. Code is available at: https://github.com/synxlin/deep-gradient-compression.}, note={arXiv: 1712.01887}, journal={arXiv:1712.01887 [cs, stat]}, author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J.}, year={2020}, month={Jun}, pages={7,10,22} }

@article{Hannun_Case_Casper_Catanzaro_Diamos_Elsen_Prenger_Satheesh_Sengupta_Coates_et, title={Deep Speech: Scaling up end-to-end speech recognition}, abstractNote={We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a “phoneme.” Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5’00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.}, note={arXiv: 1412.5567}, journal={arXiv:1412.5567 [cs]}, author={Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and et al.}, year={2014}, month={Dec}, pages={10} }

@inproceedings{Bonawitz_Ivanov_Kreuter_Marcedone_McMahan_Patel_Ramage_Segal_Seth_2017, place={New York, NY, USA}, series={CCS ’17}, title={Practical Secure Aggregation for Privacy-Preserving Machine Learning}, ISBN={978-1-4503-4946-8}, url={https://doi.org/10.1145/3133956.3133982}, DOI={10.1145/3133956.3133982}, abstractNote={We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user’s individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.}, booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security}, publisher={Association for Computing Machinery}, author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn}, year={2017}, month={Oct}, pages={10}, collection={CCS ’17} }

@article{Geyer_Klein_Nabi_2018, title={Differentially Private Federated Learning: A Client Level Perspective}, abstractNote={Federated learning is a recent advance in privacy protection. In this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. However, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client’s contribution during training and information about their data set is revealed through analyzing the distributed model. We tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients’ contributions during training, balancing the trade-off between privacy loss and model performance. Empirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance.}, note={arXiv: 1712.07557}, journal={arXiv:1712.07557 [cs, stat]}, author={Geyer, Robin C. and Klein, Tassilo and Nabi, Moin}, year={2018}, month={Mar}, pages={10} }

@inproceedings{Mao_Yi_Li_Feng_Xu_Zhong_2018, title={Learning from Differentially Private Neural Activations with Edge Computing}, DOI={10.1109/SEC.2018.00014}, abstractNote={Deep convolutional neural networks (DNNs) have brought significant performance improvements to face recognition. However the training can hardly be carried out on mobile devices because the training of these models requires much computational power. An individual user with the demand of deriving DNN models from her own datasets usually has to outsource the training procedure onto an edge server. However this outsourcing method violates privacy because it exposes the users’ data to curious service providers. In this paper, we utilize the differentially private mechanism to enable the privacy-preserving edge based training of DNN face recognition models. During the training, DNN is split between the user device and the edge server in a way that both private data and model parameters are protected, with only a small cost of local computations. We rigorously prove that our approach is privacy preserving. We finally show that our mechanism is capable of training models in different scenarios, e.g., from scratch, or through fine-tuning over existed models.}, booktitle={2018 IEEE/ACM Symposium on Edge Computing (SEC)}, author={Mao, Yunlong and Yi, Shanhe and Li, Qun and Feng, Jinghao and Xu, Fengyuan and Zhong, Sheng}, year={2018}, month={Oct}, pages={10} }

@inproceedings{Kim_Moon_2018, place={New York, NY, USA}, series={INTESA ’18}, title={Blockchain-based edge computing for deep neural network applications}, ISBN={978-1-4503-6598-7}, url={https://doi.org/10.1145/3285017.3285027}, DOI={10.1145/3285017.3285027}, abstractNote={Deep neural network (DNN) applications require heavy computations, so an embedded device with limited hardware such as an IoT device cannot run the apps by itself. One solution is to offload DNN computations from the client device to nearby edge servers [1] to request an execution of the DNN computations with their powerful hardware. However, there are several issues with the solution. One is an availability issue that how we can provide the edge server with some incentives to run the client’ apps. The other is a scalability issue that how we can use more servers when there are more DNN requests. Finally, there is an integrity issue that how the client can trust the result coming from anonymous edge servers. We think the blockchain technology can solve these issues to make edge computing more practical. This paper proposes a novel architecture for DNN edge computing based on the blockchain technology. Existing blockchains such as Ethereum do not support execution of a complex program, so we propose a modified blockchain structure and protocol to overcome the limitation.}, booktitle={Proceedings of the Workshop on INTelligent Embedded Systems Architectures and Applications}, publisher={Association for Computing Machinery}, author={Kim, Jae-Yun and Moon, Soo-Mook}, year={2018}, month={Oct}, pages={10}, collection={INTESA ’18} }
  
@article{Wang_Han_Leung_Niyato_Yan_Chen_2020, title={Convergence of Edge Computing and Deep Learning: A Comprehensive Survey}, volume={22}, ISSN={1553-877X, 2373-745X}, DOI={10.1109/COMST.2020.2970550}, number={2}, journal={IEEE Communications Surveys \& Tutorials}, author={Wang, Xiaofei and Han, Yiwen and Leung, Victor C. M. and Niyato, Dusit and Yan, Xueqiang and Chen, Xu}, year={2020}, pages={869–904} }

@inproceedings{Teerapittayanon_McDanel_Kung_2017, title={Distributed Deep Neural Networks Over the Cloud, the Edge and End Devices}, ISSN={1063-6927}, DOI={10.1109/ICDCS.2017.226}, abstractNote={We propose distributed deep neural networks (DDNNs) over distributed computing hierarchies, consisting of the cloud, the edge (fog) and end devices. While being able to accommodate inference of a deep neural network (DNN) in the cloud, a DDNN also allows fast and localized inference using shallow portions of the neural network at the edge and end devices. When supported by a scalable distributed computing hierarchy, a DDNN can scale up in neural network size and scale out in geographical span. Due to its distributed nature, DDNNs enhance sensor fusion, system fault tolerance and data privacy for DNN applications. In implementing a DDNN, we map sections of a DNN onto a distributed computing hierarchy. By jointly training these sections, we minimize communication and resource usage for devices and maximize usefulness of extracted features which are utilized in the cloud. The resulting system has built-in support for automatic sensor fusion and fault tolerance. As a proof of concept, we show a DDNN can exploit geographical diversity of sensors to improve object recognition accuracy and reduce communication cost. In our experiment, compared with the traditional method of offloading raw sensor data to be processed in the cloud, DDNN locally processes most sensor data on end devices while achieving high accuracy and is able to reduce the communication cost by a factor of over 20x.}, booktitle={2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)}, author={Teerapittayanon, Surat and McDanel, Bradley and Kung, H.T.}, year={2017}, month={Jun}, pages={328–339} }

@article{Nweke_Teh_Al-garadi_Alo_2018, title={Deep learning algorithms for human activity recognition using mobile and wearable sensor networks: State of the art and research challenges}, volume={105}, ISSN={0957-4174}, DOI={10.1016/j.eswa.2018.03.056}, abstractNote={Human activity recognition systems are developed as part of a framework to enable continuous monitoring of human behaviours in the area of ambient assisted living, sports injury detection, elderly care, rehabilitation, and entertainment and surveillance in smart home environments. The extraction of relevant features is the most challenging part of the mobile and wearable sensor-based human activity recognition pipeline. Feature extraction influences the algorithm performance and reduces computation time and complexity. However, current human activity recognition relies on handcrafted features that are incapable of handling complex activities especially with the current influx of multimodal and high dimensional sensor data. With the emergence of deep learning and increased computation powers, deep learning and artificial intelligence methods are being adopted for automatic feature learning in diverse areas like health, image classification, and recently, for feature extraction and classification of simple and complex human activity recognition in mobile and wearable sensors. Furthermore, the fusion of mobile or wearable sensors and deep learning methods for feature learning provide diversity, offers higher generalisation, and tackles challenging issues in human activity recognition. The focus of this review is to provide in-depth summaries of deep learning methods for mobile and wearable sensor-based human activity recognition. The review presents the methods, uniqueness, advantages and their limitations. We not only categorise the studies into generative, discriminative and hybrid methods but also highlight their important advantages. Furthermore, the review presents classification and evaluation procedures and discusses publicly available datasets for mobile sensor human activity recognition. Finally, we outline and explain some challenges to open research problems that require further research and improvements.}, journal={Expert Systems with Applications}, author={Nweke, Henry Friday and Teh, Ying Wah and Al-garadi, Mohammed Ali and Alo, Uzoma Rita}, year={2018}, month={Sep}, pages={233–261} }

@inproceedings{Ogden_Guo_2018, title={{MODI}: Mobile Deep Inference Made Efficient by Edge Computing}, url={https://www.usenix.org/conference/hotedge18/presentation/ogden}, author={Ogden, Samuel S. and Guo, Tian}, year={2018} }
 
@article{McDanel_Teerapittayanon_Kung_2017, title={Embedded Binarized Neural Networks}, url={http://arxiv.org/abs/1709.02260}, abstractNote={We study embedded Binarized Neural Networks (eBNNs) with the aim of allowing current binarized neural networks (BNNs) in the literature to perform feedforward inference efficiently on small embedded devices. We focus on minimizing the required memory footprint, given that these devices often have memory as small as tens of kilobytes (KB). Beyond minimizing the memory required to store weights, as in a BNN, we show that it is essential to minimize the memory used for temporaries which hold intermediate results between layers in feedforward inference. To accomplish this, eBNN reorders the computation of inference while preserving the original BNN structure, and uses just a single floating-point temporary for the entire neural network. All intermediate results from a layer are stored as binary values, as opposed to floating-points used in current BNN implementations, leading to a 32x reduction in required temporary space. We provide empirical evidence that our proposed eBNN approach allows efficient inference (10s of ms) on devices with severely limited memory (10s of KB). For example, eBNN achieves 95% accuracy on the MNIST dataset running on an Intel Curie with only 15 KB of usable memory with an inference runtime of under 50 ms per sample. To ease the development of applications in embedded contexts, we make our source code available that allows users to train and discover eBNN models for a learning task at hand, which fit within the memory constraint of the target device.}, note={arXiv: 1709.02260}, journal={arXiv:1709.02260 [cs]}, author={McDanel, Bradley and Teerapittayanon, Surat and Kung, H. T.}, year={2017}, month={Sep} }

@article{Courbariaux_Hubara_Soudry_El-Yaniv_Bengio_2016, title={Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1}, url={http://arxiv.org/abs/1602.02830}, abstractNote={We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.}, note={arXiv: 1602.02830}, journal={arXiv:1602.02830 [cs]}, author={Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua}, year={2016}, month={Mar} }

@inproceedings{Mao_Chen_Nixon_Krieger_Chen_2017, place={Lausanne, Switzerland}, title={MoDNN: Local distributed mobile computing system for Deep Neural Network}, ISBN={978-3-9815370-8-6}, url={http://ieeexplore.ieee.org/document/7927211/}, DOI={10.23919/DATE.2017.7927211}, booktitle={Design, Automation \& Test in Europe Conference \& Exhibition (DATE), 2017}, publisher={IEEE}, author={Mao, Jiachen and Chen, Xiang and Nixon, Kent W. and Krieger, Christopher and Chen, Yiran}, year={2017}, month={Mar}, pages={1396–1401} }

 @inproceedings{Stahl_Zhao_Mueller-Gritschneder_Gerstlauer_Schlichtmann_2019, place={Cham}, series={Lecture Notes in Computer Science}, title={Fully Distributed Deep Learning Inference on Resource-Constrained Edge Devices}, ISBN={978-3-030-27562-4}, DOI={10.1007/978-3-030-27562-4_6}, abstractNote={Performing inference tasks of deep learning applications on IoT edge devices ensures privacy of input data and can result in shorter latency when compared to a cloud solution. As most edge devices are memory- and compute-constrained, they cannot store and execute a complete Deep Neural Network (DNN). One possible solution is to distribute the DNN across multiple edge devices. For a complete distribution, both fully-connected and feature- and weight-intensive convolutional layers need to be partitioned to reduce the amount of computation and data on each resource-constrained edge device. At the same time, resulting communication overheads need to be considered. Existing work on distributed DNN execution can not support all types of networks and layers or does not account for layer fusion opportunities to reduce communication. In this paper, we jointly optimize memory, computation and communication demands for distributed execution of complete neural networks covering all layers. This is achieved through techniques that combine both feature and weight partitioning with a communication-aware layer fusion approach to enable holistic optimization across layers. For a given number of edge devices, the schemes are applied jointly such that the amount of data to be exchanged between devices is minimized to optimize run time.Experimental results for a simulation of six edge devices on 100 Mbit connections running the YOLOv2 DNN model show that the schemes evenly balance the memory footprint between devices. The integration of layer fusion additionally leads to a reduction of communication demands by 14.8%. This results in run time speed-up of the inference task by 1.15x compared to partitioning without fusing.}, booktitle={Embedded Computer Systems: Architectures, Modeling, and Simulation}, publisher={Springer International Publishing}, author={Stahl, Rafael and Zhao, Zhuoran and Mueller-Gritschneder, Daniel and Gerstlauer, Andreas and Schlichtmann, Ulf}, editor={Pnevmatikatos, Dionisios N. and Pelcat, Maxime and Jung, Matthias}, year={2019}, pages={77–90}, collection={Lecture Notes in Computer Science} }

 @inproceedings{Hadidi_Cao_Ryoo_Kim_2019, place={Las Vegas NV USA}, title={Robustly Executing DNNs in IoT Systems Using Coded Distributed Computing}, ISBN={978-1-4503-6725-7}, url={https://dl.acm.org/doi/10.1145/3316781.3322474}, DOI={10.1145/3316781.3322474}, booktitle={Proceedings of the 56th Annual Design Automation Conference 2019}, publisher={ACM}, author={Hadidi, Ramyad and Cao, Jiashen and Ryoo, Michael S. and Kim, Hyesoon}, year={2019}, month={Jun}, pages={1–2} }


@article{Hadidi_8411096,  author={Hadidi, Ramyad and Cao, Jiashen and Woodward, Matthew and Ryoo, Michael S. and Kim, Hyesoon}, journal={IEEE Robotics and Automation Letters}, title={Distributed Perception by Collaborative Robots},   year={2018}, volume={3}, number={4}, pages={3709-3716}, abstract={Recognition ability and, more broadly, machine learning techniques enable robots to perform complex tasks and allow them to function in diverse situations. In fact, robots can easily access an abundance of sensor data that are recorded in real time such as speech, image, and video. Since such data are time sensitive, processing them in real time is a necessity. Moreover, machine learning techniques are known to be computationally intensive and resource hungry. As a result, an individual resource-constrained robot, in terms of computation power and energy supply, is often unable to handle such heavy real-time computations alone. To overcome this obstacle, we propose a framework to harvest the aggregated computational power of several low-power robots for enabling efficient, dynamic, and real-time recognition. Our method adapts to the availability of computing devices at runtime and adjusts to the inherit dynamics of the network. Our framework can be applied to any distributed robot system. To demonstrate, with several Raspberry-Pi3-based robots (up to 12) each equipped with a camera, we implement a state-of-the-art action recognition model for videos and two recognition models for images. Our approach allows a group of multiple low-power robots to obtain a similar performance (in terms of the number of images or video frames processed per second) compared to a high-end embedded platform, Nvidia Tegra TX2.},  keywords={},  doi={10.1109/LRA.2018.2856261},  ISSN={2377-3766},  month={Oct} }

@inproceedings{Ngiam_Khosla_Kim_Nam_Lee_Ng_2011, place={Madison, WI, USA}, series={ICML’11}, title={Multimodal deep learning}, ISBN={978-1-4503-0619-5}, abstractNote={Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.}, booktitle={Proceedings of the 28th International Conference on International Conference on Machine Learning}, publisher={Omnipress}, author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y.}, year={2011}, month={Jun}, pages={689–696}, collection={ICML’11} }

@article{Ditzler_Roveri_Alippi_Polikar_2015, title={Learning in Nonstationary Environments: A Survey}, volume={10}, ISSN={1556-6048}, DOI={10.1109/MCI.2015.2471196}, abstractNote={The prevalence of mobile phones, the internet-of-things technology, and networks of sensors has led to an enormous and ever increasing amount of data that are now more commonly available in a streaming fashion [1]-[5]. Often, it is assumed - either implicitly or explicitly - that the process generating such a stream of data is stationary, that is, the data are drawn from a fixed, albeit unknown probability distribution. In many real-world scenarios, however, such an assumption is simply not true, and the underlying process generating the data stream is characterized by an intrinsic nonstationary (or evolving or drifting) phenomenon. The nonstationarity can be due, for example, to seasonality or periodicity effects, changes in the users’ habits or preferences, hardware or software faults affecting a cyber-physical system, thermal drifts or aging effects in sensors. In such nonstationary environments, where the probabilistic properties of the data change over time, a non-adaptive model trained under the false stationarity assumption is bound to become obsolete in time, and perform sub-optimally at best, or fail catastrophically at worst.}, number={4}, journal={IEEE Computational Intelligence Magazine}, author={Ditzler, Gregory and Roveri, Manuel and Alippi, Cesare and Polikar, Robi}, year={2015}, month={Nov}, pages={12–25} }

@inproceedings{Huai_Ding_Wang_Geng_Zhang_2019, place={Leicester, United Kingdom}, title={Towards Deep Learning on Resource-Constrained Robots: A Crowdsourcing Approach with Model Partition}, ISBN={978-1-72814-034-6}, url={https://ieeexplore.ieee.org/document/9060297/}, DOI={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00194}, booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence \& Computing, Advanced \& Trusted Computing, Scalable Computing \& Communications, Cloud \& Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)}, publisher={IEEE}, author={Huai, Zhibo and Ding, Bo and Wang, Huaimin and Geng, Mingyang and Zhang, Lei}, year={2019}, month={Aug}, pages={989–994} }

@article{Peteiro-Barral_Guijarro-Berdiñas_2013, title={A survey of methods for distributed machine learning}, volume={2}, ISSN={2192-6352, 2192-6360}, DOI={10.1007/s13748-012-0035-5}, number={1}, journal={Progress in Artificial Intelligence}, author={Peteiro-Barral, Diego and Guijarro-Berdiñas, Bertha}, year={2013}, month={Mar}, pages={1–11} }

@article{Ribeiro_Singh_Guestrin_2016, title={“Why Should I Trust You?”: Explaining the Predictions of Any Classifier}, url={http://arxiv.org/abs/1602.04938}, abstractNote={Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.}, note={arXiv: 1602.04938}, journal={arXiv:1602.04938 [cs, stat]}, author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos}, year={2016}, month={Aug} }

@article{Adadi_Berrada_2018, title={Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}, volume={6}, ISSN={2169-3536}, DOI={10.1109/ACCESS.2018.2870052}, journal={IEEE Access}, author={Adadi, Amina and Berrada, Mohammed}, year={2018}, pages={52138–52160} }

@article{Simonite, title={The Best Algorithms Still Struggle to Recognize Black Faces}, ISSN={1059-1028}, url={https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/}, abstractNote={US government tests find even top-performing facial recognition systems misidentify black people at rates 5 to 10 times higher than they do white people.}, journal={Wired}, author={Simonite, Tom} }

@article{Rotem_Fix_Abdulrasool_Catron_Deng_Dzhabarov_Gibson_Hegeman_Lele_Levenstein_et, title={Glow: Graph Lowering Compiler Techniques for Neural Networks}, url={http://arxiv.org/abs/1805.00907}, abstractNote={This paper presents the design of Glow, a machine learning compiler for heterogeneous hardware. It is a pragmatic approach to compilation that enables the generation of highly optimized code for multiple targets. Glow lowers the traditional neural network dataflow graph into a two-phase strongly-typed intermediate representation. The high-level intermediate representation allows the optimizer to perform domain-specific optimizations. The lower-level instruction-based address-only intermediate representation allows the compiler to perform memory-related optimizations, such as instruction scheduling, static memory allocation and copy elimination. At the lowest level, the optimizer performs machine-specific code generation to take advantage of specialized hardware features. Glow features a lowering phase which enables the compiler to support a high number of input operators as well as a large number of hardware targets by eliminating the need to implement all operators on all targets. The lowering phase is designed to reduce the input space and allow new hardware backends to focus on a small number of linear algebra primitives.}, note={arXiv: 1805.00907}, journal={arXiv:1805.00907 [cs]}, author={Rotem, Nadav and Fix, Jordan and Abdulrasool, Saleem and Catron, Garret and Deng, Summer and Dzhabarov, Roman and Gibson, Nick and Hegeman, James and Lele, Meghan and Levenstein, Roman and et al.}, year={2019}, month={Apr} }
 
@inproceedings{Friedler_Scheidegger_Venkatasubramanian_Choudhary_Hamilton_Roth_2019, place={Atlanta GA USA}, title={A comparative study of fairness-enhancing interventions in machine learning}, ISBN={978-1-4503-6125-5}, url={https://dl.acm.org/doi/10.1145/3287560.3287589}, DOI={10.1145/3287560.3287589}, booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency}, publisher={ACM}, author={Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek}, year={2019}, month={Jan}, pages={329–338} }

@article{Li_Liu_Liu_Sun_You_Yang_Luan_Gan_Yang_Qian_2021, title={The Deep Learning Compiler: A Comprehensive Survey}, volume={32}, ISSN={1045-9219, 1558-2183, 2161-9883}, DOI={10.1109/TPDS.2020.3030548}, abstractNote={The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this paper, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. Specifically, we provide a comprehensive comparison among existing DL compilers from various aspects. In addition, we present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey paper focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.}, note={arXiv: 2002.03794}, number={3}, journal={IEEE Transactions on Parallel and Distributed Systems}, author={Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei}, year={2021}, month={Mar}, pages={708–727} }

@article{Biswas_Rajan_2020, title={Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias? An Empirical Study on Model Fairness}, DOI={10.1145/3368089.3409704}, abstractNote={Machine learning models are increasingly being used in important decision-making software such as approving bank loans, recommending criminal sentencing, hiring employees, and so on. It is important to ensure the fairness of these models so that no discrimination is made based on protected attribute (e.g., race, sex, age) while decision making. Algorithms have been developed to measure unfairness and mitigate them to a certain extent. In this paper, we have focused on the empirical evaluation of fairness and mitigations on real-world machine learning models. We have created a benchmark of 40 top-rated models from Kaggle used for 5 different tasks, and then using a comprehensive set of fairness metrics, evaluated their fairness. Then, we have applied 7 mitigation techniques on these models and analyzed the fairness, mitigation results, and impacts on performance. We have found that some model optimization techniques result in inducing unfairness in the models. On the other hand, although there are some fairness control mechanisms in machine learning libraries, they are not documented. The mitigation algorithm also exhibit common patterns such as mitigation in the post-processing is often costly (in terms of performance) and mitigation in the pre-processing stage is preferred in most cases. We have also presented different trade-off choices of fairness mitigation decisions. Our study suggests future research directions to reduce the gap between theoretical fairness aware algorithms and the software engineering methods to leverage them in practice.}, note={arXiv: 2005.12379}, journal={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, author={Biswas, Sumon and Rajan, Hridesh}, year={2020}, month={Nov}, pages={642–653} }
